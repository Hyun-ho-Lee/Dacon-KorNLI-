{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled70.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "gwBftuvPsMMY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sUKrrWZsK-u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import random\n",
        "import numpy as np \n",
        "import os\n",
        "import re\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tqdm import tqdm\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AdamW,RobertaForSequenceClassification\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from tensorboardX import SummaryWriter "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = False  # type: ignore\n",
        "\n",
        "def seed_worker(_worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "    \n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "69Yne-2dsQp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load"
      ],
      "metadata": {
        "id": "IvIAcFcFsTZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(train='train_data.csv', test='test_data.csv'):\n",
        "    train = pd.read_csv('../data/' + train)\n",
        "    test = pd.read_csv('../data/' + test)\n",
        "    submission = pd.read_csv('../data/sample_submission.csv')\n",
        "    train_1 =pd.read_csv('../data/train_dev.csv')\n",
        "    train_2 =pd.read_csv('../data/new_df.csv')\n",
        "    train = pd.concat([train,train_1,train_2], ignore_index=True)\n",
        "    label_dict = {\"entailment\" : 0, \"contradiction\" : 1, \"neutral\" : 2}\n",
        "    train['label'] = train['label'].map(label_dict)\n",
        "    return train, test, submission\n",
        "\n",
        "def concat(df):\n",
        "    df[\"premise_\"] = \"[CLS]\" + df[\"premise\"] + \"[SEP]\"\n",
        "    df[\"hypothesis_\"] = df[\"hypothesis\"] + \"[SEP]\"\n",
        "    df[\"text_sum\"] = df.premise_ + \"\" + df.hypothesis_\n",
        "    df = df[['text_sum','label']]\n",
        "    return df"
      ],
      "metadata": {
        "id": "TM6vbmpjsRwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test, submission = load_data()\n",
        "train.drop_duplicates(inplace = True)\n",
        "train, test  = concat(train), concat(test)  \n",
        "train, valid = train_test_split(train, stratify=train['label'], random_state=42)"
      ],
      "metadata": {
        "id": "H6ZPbq5ZsWSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting"
      ],
      "metadata": {
        "id": "9vBNbiuIsZJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "EPOCHS = 32\n",
        "batch_size = 10\n",
        "lr = 0.00001\n",
        "warmup_ratio = 0.1\n",
        "pretrain = \"klue/bert-base\""
      ],
      "metadata": {
        "id": "C0UIf4-CsX3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CustomDataset"
      ],
      "metadata": {
        "id": "VXDKWiKhscbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset, option):\n",
        "        self.dataset = dataset \n",
        "        self.option = option\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(pretrain)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "  \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataset.iloc[idx, 0:2].values\n",
        "        text = row[0]\n",
        "    \n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            max_length=100,\n",
        "            pad_to_max_length=True,\n",
        "            add_special_tokens=False,\n",
        "            truncation = True)\n",
        "        \n",
        "        input_ids = inputs['input_ids'][0]\n",
        "        attention_mask = inputs['attention_mask'][0]\n",
        "        \n",
        "        if self.option =='train':\n",
        "            label = row[1]\n",
        "            return input_ids, attention_mask, label\n",
        "        \n",
        "        return input_ids, attention_mask"
      ],
      "metadata": {
        "id": "DuMA8e5fsaqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(pretrain)\n",
        "model = AutoModel.from_pretrained(\"klue/bert-base\",num_labels = 3).to(device)\n",
        "\n",
        "train_dataset = CustomDataset(train, 'train')\n",
        "valid_dataset = CustomDataset(valid, 'train')\n",
        "test_dataset = CustomDataset(test, 'test')\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, worker_init_fn=seed_worker, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=8, worker_init_fn=seed_worker, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, worker_init_fn=seed_worker, pin_memory=True)"
      ],
      "metadata": {
        "id": "GzC5gXKHse7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_steps = len(train_loader) * EPOCHS\n",
        "warmup_step = int(total_steps * warmup_ratio)\n",
        "\n",
        "model = nn.DataParallel(model).to(device)\n",
        "optimizer = AdamW(model.parameters(), weight_decay=1e-4,correct_bias=False,lr=lr)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=1, num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "id": "M7dTbfbJsg5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits = 5,shuffle=True,random_state=42)\n",
        "folds=[]\n",
        "for trn_idx,val_idx in skf.split(train['text_sum'],train['label']):\n",
        "    folds.append((trn_idx,val_idx))"
      ],
      "metadata": {
        "id": "ZetM7d6AthA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & Validation "
      ],
      "metadata": {
        "id": "SCYnuO6uskuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_models = []\n",
        "\n",
        "writer = SummaryWriter(logdir='/daintlab/home/ddualab/hyunho/tensorboard') \n",
        "\n",
        "for i,fold in enumerate(range(0,5)):\n",
        "    print('===============',i+1,'fold start===============')\n",
        "    model = RobertaForSequenceClassification.from_pretrained(pretrain,num_labels=3).to(device)\n",
        "    model=nn.DataParallel(model).to(device)\n",
        "    optimizer = AdamW(model.parameters(),lr=lr,weight_decay=1e-4,correct_bias=False)\n",
        "    \n",
        "    \n",
        "    train_idx = folds[fold][0]\n",
        "    valid_idx = folds[fold][1]\n",
        "    train_data = train.loc[train_idx]\n",
        "    val_data = train.loc[valid_idx]\n",
        "    train_dataset = CustomDataset(train_data,'train')\n",
        "    valid_dataset = CustomDataset(val_data,'train')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "    warmup_ratio = 0.1\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    warmup_step = int(total_steps * warmup_ratio)\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=1, num_training_steps=total_steps)\n",
        "    val_acc_list = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print('===============',epoch+1,'epoch start===============')\n",
        "        batches = 0\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total =0\n",
        "        model.train()\n",
        "        \n",
        "        for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred = model(input_ids_batch.to(device), attention_mask = attention_masks_batch.to(device))[0]\n",
        "            loss = F.cross_entropy(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(y_pred, 1)\n",
        "            correct += (predicted == y_batch).sum()\n",
        "            total += len(y_batch)\n",
        "            batches += 1\n",
        "            if batches % 100 == 0:\n",
        "                print(\"Batch Loss: \", total_loss, \"Accuracy: \", correct.float() / total)\n",
        "            writer.add_scalar('train/train_loss',total_loss,epoch)\n",
        "            writer.add_scalar('train/train_acc',correct,epoch)\n",
        "        val_loss = []\n",
        "        val_acc = []\n",
        "        \n",
        "        for input_ids_batch, attention_masks_batch, y_batch in tqdm(valid_loader):\n",
        "            \n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                \n",
        "                y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
        "                valid_loss = F.cross_entropy(y_pred,y_batch.to(device)).cpu().detach().numpy()\n",
        "\n",
        "                preds = torch.argmax(y_pred,1)\n",
        "                preds = preds.cpu().detach().numpy()\n",
        "                y_batch = y_batch.cpu().detach().numpy()\n",
        "                batch_acc = (preds==y_batch).mean()\n",
        "                val_loss.append(valid_loss)\n",
        "                val_acc.append(batch_acc)\n",
        "                \n",
        "                \n",
        "        val_loss = np.mean(val_loss)\n",
        "        val_acc = np.mean(val_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "        scheduler.step()\n",
        "        print(f'Epoch: {epoch}- valid Loss: {val_loss:.6f} - valid_acc : {val_acc:.6f}')\n",
        "        print(optimizer.param_groups[0][\"lr\"])\n",
        "        writer.add_scalar('val/val_loss',val_loss,epoch)\n",
        "        writer.add_scalar('val/val_acc',val_acc,epoch)\n",
        "        if max(val_acc_list) <= val_acc:\n",
        "            torch.save(model.state_dict(), f'./models/Roberta_large_fold_{i}_{val_acc}.pth') \n",
        "            print('model save, model val acc : ',val_acc)\n",
        "            print('best_models size : ',len(best_models))\n",
        "\n",
        "\n",
        "torch.save(model, './models/Roberta_large_model.pt')"
      ],
      "metadata": {
        "id": "6I-ZE5WntEUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "z-SZoDcptHO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset(test,'test')\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in range(5): \n",
        "    print(f'fold{i} Start')\n",
        "    model = torch.load('./models/Roberta_large_model.pt')\n",
        "  \n",
        "    model.load_state_dict(torch.load(f'./models/best/Roberta_large_fold_{i}.pth'))\n",
        "    model.eval()\n",
        "    answer = []\n",
        "    with torch.no_grad():\n",
        "        for input_ids_batch, attention_masks_batch in tqdm(test_loader):\n",
        "            y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0].detach().cpu().numpy()\n",
        "            answer.extend(y_pred.argmax(axis=1))\n",
        "    preds.append(answer)\n",
        "\n",
        "np_pred = np.array(preds).T\n",
        "\n",
        "pred = []\n",
        "for i in range(1666):\n",
        "    cnt = Counter(np_pred[i])\n",
        "    pred.append(cnt.most_common()[0][0])"
      ],
      "metadata": {
        "id": "xRzPgnNEtIqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission"
      ],
      "metadata": {
        "id": "I-UjGHietL_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission['label']=pred\n",
        "label_dict1 = {0:\"entailment\" , 1: \"contradiction\" , 2:\"neutral\"}\n",
        "submission['label']=submission['label'].map(label_dict1)\n",
        "submission.to_csv('./predict/Roberta_Large_real_final_model.csv',index=False)"
      ],
      "metadata": {
        "id": "FeA1ox2QtLSw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}